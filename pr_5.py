# -*- coding: utf-8 -*-
"""PR_5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xkVtywxfG3ECwu_HCl8Y2pNJGbqOUq0y
"""

pip install graphviz

import numpy as np
import pandas as pd
import numpy as np, pandas as pd
import matplotlib.pyplot as plt
from sklearn import metrics, model_selection, preprocessing
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import export_graphviz
import graphviz

data = pd.read_csv('C:/Users/Admin/Downloads/car_evaluation.csv',names=['buying','maint','doors','lug_boot','safety','class'])

data.head()

data.info()

#Identify the target variable

data['class'],class_names = pd.factorize(data['class'])

print(class_names)
print(data['class'].unique())

#Identify the predictor variables and encode any string variables to equivalent integer codes
data['buying'],_ = pd.factorize(data['buying'])
data['maint'],_ = pd.factorize(data['maint'])
data['doors'],_ = pd.factorize(data['doors'])
data['lug_boot'],_ = pd.factorize(data['lug_boot'])
data['safety'],_ = pd.factorize(data['safety'])
data.head()

data.info()

#Select the predictor feature and select the target variable

X = data.iloc[:,:-1]
y = data.iloc[:,-1]

# split data randomly into 70% training and 30% test
X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=0)

# we can achieve the above two tasks using the following codes
# Bagging: using all features
model = RandomForestClassifier(random_state=1)
model.fit(X_train, y_train)

# use the model to make predictions with the test data
y_pred = model.predict(X_test)
# how did our model perform?
count_misclassified = (y_test != y_pred).sum()
print('Misclassified samples: {}'.format(count_misclassified))
accuracy = metrics.accuracy_score(y_test, y_pred)
print('Accuracy: {:.2f}'.format(accuracy))

# create the classifier with n_estimators = 100

clf = RandomForestClassifier(n_estimators=100, random_state=0)

# fit the model to the training set

clf.fit(X_train, y_train)

feature_scores = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)

feature_scores

# Creating a seaborn bar plot
import seaborn as sns
sns.barplot(x=feature_scores, y=feature_scores.index)
# Add labels to the graph
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
# Add title to the graph
plt.title("Visualizing Important Features")
# Visualize the graph
plt.show()

# Find the most important feature
feature_importances = clf.feature_importances_
best_feature_index = np.argmax(feature_importances)
best_feature_name = X_train.columns[best_feature_index]

#Print the name of the best feature
print(f"The best feature is: {best_feature_name}")

# Create a decision tree using the best feature
best_feature_data = X_train.iloc[:, best_feature_index].values
tree_classifier = RandomForestClassifier(n_estimators=1, random_state=0)  # Create a single tree
tree_classifier.fit(best_feature_data.reshape(-1, 1), y_train)  # Reshape the data for a single feature

# Visualize the decision tree
dot_data = export_graphviz(tree_classifier.estimators_[0], out_file=None,
                           feature_names=[best_feature_name], class_names=class_names,
                           filled=True, rounded=True, special_characters=True)

graph = graphviz.Source(dot_data)
graph.render("C:/Users/Admin/Desktop/ML/PR_5",format="pdf")  # Save the tree to a file
graph.view("best_feature_tree")  # Open the tree in a viewer

